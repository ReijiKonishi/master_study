# 提案モデルの定義

提案モデルにおける変数は、離散変数と連続変数に分けられ、離散変数は0以上の整数を取る確率変数であると仮定する。

1. $p$個の観測変数$X = \{ X_1, \dots, X_p \}$はDAGによって表現されるデータ生成仮定から生成されており、
   各変数の親変数がその変数の直接的な原因である。

2. 連続変数に割り当てられた変数$X_j$は
   その親変数$Pa(j)$と誤差変数$e_j$の線形和である。
   $$
   \begin{equation}
       X_j = e_j + \theta_{j} + \sum_{k \in Pa(j)} \theta_{jk}X_j
       \quad \text{with} \quad e_j \sim (0, \sigma_j^2)
   \end{equation}
   $$
   それぞれの係数$\theta_{jk}$は、変数$X_k$から変数$X_j$への直接的な因果効果の大きさを表す。
   また、誤差変数$e_j$は任意の確率分布に従う確率変数であり、お互いに独立である。

   ※正規化後のデータ生成過程をここに書いて、その$\sigma$に対して仮定を入れる

   ※モデルは単純にAdditive Noise Modelで記述

   ※誤差分散に関する仮定を設けて、その仮定の下での識別可能性を証明
   
3. 離散変数に割り当てられた変数$X_j$は、
   その親変数$Pa(j)$による条件付き確率が、2次分散関数性を満たす。
   つまり、以下を満たすような$\beta_{j0},\beta_{j1} \in \mathbb{R}$が存在する。
   $$
   \begin{equation}
       \mathit{Var}(X_j|X_{Pa(j)}) = \beta_{j0} E(X_j | X_{Pa(j)}) + \beta_{j1} E(X_j | X_{Pa(j)})^2
   \end{equation}
   $$
   また、各変数の条件付き期待値は、その変数の親変数$Pa(j)$と
   任意の単調で微分可能なリンク関数$g_j \colon \mathcal X_{Pa(j)} \rightarrow \mathbb R^+$によって以下のように記述される。
   $$
   \begin{equation}
    E(X_j | X_{Pa(j)})
    = g_j(X_{Pa(j)})
    = g_j \left(\theta_j + \sum_{k \in Pa(j)} \theta_{jk}X_k \right)
    \end{equation}
   $$

4. 



※モデル自体は全体的に線形で書いてるが、識別可能条件に線形性は必要ない







# QVF-DAGの命題が提案モデルでも成り立つ

## 命題

提案モデルにおいて、
  離散変数が割り当てあられた任意の頂点$j \in D$、任意の集合$S_j \subset Nd(j)$に関して、
  以下のモーメント関係が成立している。
$$
\begin{equation}
    \frac{E(X_j^2)}
    {E \left[ \beta_0 E(X_j | X_{S_j}) + (\beta_1 + 1)E(X_j | X_{S_j})^2 \right]}
    \geq 1
  \end{equation}
$$
  等号成立は、$S_j$が頂点$j \in D$の親変数全てを含むとき($Pa(j) \subset S_j$)である。



## 証明

任意の変数$X_j \in X_D$について以下のように表現できる。(以下のような$\beta_1$が存在する。)
$$
\begin{alignat*}{2}
    \mathit{Var}(X) &= E(X^2) - E(X)^2 & \qquad & \text{分散の公式より} \\
                    &= \beta_0 E(X) + \beta_1 E(X)^2 && \text{2次分散関数性の定義より}
\end{alignat*}
$$
よって、
$$
\begin{equation*}
    E(X_j^2) = \beta_0 E(X_j) + (\beta_1 + 1)E(X_j)^2
\end{equation*}
$$



ここで、記号の簡単の簡単のために、関数$f(\mu) = \beta \mu + (\beta_1 + 1) \mu^2$を定義する。

すると、任意の頂点$j \in D$、任意の空でない集合$S_j \subset Nd(j)$について、以下のように書ける。
$$
\begin{equation}
    \begin{split}
      E(X_j^2 | S_j) &= E(E(X_j^2 | X_{Pa(j)}) | S_j) \\
                     &= E(f(E(X_j | X_{Pa(j)})) | S_j)
    \end{split}
  \end{equation}
$$

提案モデルにおいては連続変数と離散変数が混在するモデルを考えているため、
離散変数が割り当てられた頂点$j \in D$の非子孫の集合$Nd(j)$には、
連続変数と離散変数の両方が含まれている可能性がある。

つまり、$S_j$に連続変数と離散変数のどちらが含まれていても成立する。

以降の証明は、QVF-DAGの証明と同様。







# 例



「$Y$ で条件付けると、$X$と$Z$は独立」という条件付き独立関係が成り立っっている
→以下の3つのグラフはマルコフ同値。


$$
\begin{align*}
  G_1 \colon & X = \theta_{X} + e_X, \quad e_X \sim N(0, \sigma_X^2) \\
             & Y|X \sim \mathit{Poisson}(\lambda), \quad \log(\lambda) = \theta_Y + \theta_{YX}X \\
             & Z = \theta_Z + \theta_{ZY}Y + e_Z, \quad e_Z \sim N(0, \sigma_Z^2)
\end{align*}
$$



$$
\begin{align*}
  G_2 \colon & X = \theta_X + \theta_{XY}Y + e_X, \quad e_X \sim N(0, \sigma_X^2) \\
             & Y|Z \sim \mathit{Possion}(\lambda), \quad \log(\lambda) = \theta_Y + \theta_{YZ}Z \\
             & Z = \theta_{Z} + e_Z, \quad e_Z \sim N(0, \sigma_Z^2)
\end{align*}
$$



$$
\begin{align*}
  G_3 \colon & X = \theta_X + \theta_{XY}Y + e_X, \quad e_X \sim N(0, \sigma_X^2) \\
             & Y \sim \mathit{Poisson}(\lambda) \\
             & Z = \theta_{Z} + e_Z, \quad e_Z \sim N(0, \sigma_Z^2)
\end{align*}
$$



<img src="/Users/reiji/Pictures/screenshot/スクリーンショット 2020-12-22 9.27.22.png" alt="スクリーンショット 2020-12-22 9.27.22" style="zoom:100%;" />



* 先程の命題より、$G_1, G_2$においては、
  $$
  E(Y^2) > E(Y) + E(Y)^2
  $$

* 一方で、$G_3$においては、
  $$
  E(Y^2) = E(Y) + E(Y)^2
  $$
  
* よって、離散変数のモーメント比が1か1以上かを調べることで、$G_1, G_2$ と $G_3$は識別可能



* $G_1$ について、もし連続変数$X,Z$の誤差変数の分散が、$\sigma_X^2 < \sigma_Z^2 + \mathit{Var}(E(Z|Y))$ を満たすならば、
  全分散の公式を用いて、以下が成り立つ

$$
\begin{align*}
  \mathit{Var}(Z) &= E(\mathit{Var}(Z|Y)) + \mathit{Var}(E(Z|Y)) \\
                  &= \sigma_Z^2 + \mathit{Var}(E(Z|Y)) \\
                  &> \sigma_X^2 \\
                  &= \mathit{Var}(X)
\end{align*}
$$

* よって、$X$のほうが因果順序が早いことが分かる





# 提案モデルの識別可能性

## 定理

提案したDAGモデルは、以下の仮定を満たすとき識別可能である。
ここで、$\pi$はDAG $G$における因果順序を表す。


* 連続変数が割り当てられた任意の頂点$j = \pi_m \in C, k \in De(j) \subset C$の
  データ生成過程における誤差変数の分散について、以下が満たされている。
  
  
  $$
  \begin{equation*}
        \sigma_j^2 < \sigma_k^2 + E(\mathit{Var}(E(X_k | X_{Pa(k)}) | X_{\pi_1}, \dots, X_{\pi_{m-1}}))
    \end{equation*}
  $$
  
  * かなり強い仮定を置いている…
  
  * 分布の形状は問わないので、正規分布でもOK
  
  * $\sigma_j = \sigma_k$ のときは常に成り立つ
  
  * 「正規化されたデータに対してこれが成り立っているという仮定にする」
  
  * 正規化後の$\sigma$は別の記号を使う
    元の$\sigma$を使ってこの条件を書き直すのでもOK
  
    
  
* 離散変数が割り当てあられた任意の頂点$j \in D$について、$\beta_{j1} > -1$が満たされている


    * 識別可能でないことが知られているベルヌーイ分布や多項分布によるDAGモデルを除外している





## 証明

数学的帰納法を用いる

### Step(1)

#### $\pi_1 = j \in D$の場合

* 先程の命題より、$E(X_{\pi_1}^2) = E(f(E(X_{\pi_1})))$が成立する。
* 一方で、頂点$j \in D \backslash \{\pi_1\}$では、$E(X_j^2) > E(f(E(X_j)))$となる。
* そのため、因果順序が1番目の要素$\pi_1$は、$E(X_j^2) = E(f(E(X_j)))$となるような$j \in D$である。
* もし、そのような変数が存在しなければ、$X_{\pi_1}$は連続変数である。



#### $\pi_1 = j \in C$の場合

* 仮定により、任意の頂点$k \in C \backslash \{\pi_1 \}$について、以下が成立する。

$$
\begin{align*}
        \mathit{Var}(X_{\pi_1}) &= \sigma_{\pi_1}^2 \\
                                &< \sigma_k^2 + \mathit{Var}(E(X_k | X_{Pa(k)})) \\
                                &= E(\mathit{Var}(X_k | X_{Pa(k)})) + \mathit{Var}(E(X_k | X_{Pa(k)})) \\
                                &= \mathit{Var}(X_k)
\end{align*}
$$

* よって、因果順序が一番目の要素を特定することができる。



### Step(m-1)

* 因果順序が$m-1$番目まで正しく推定されていると仮定する。
* 故にm-1で、hogehogeが成立するということを書く



### Step(m)

#### $\pi_m = j \in D$の場合

* 先程の命題より、$E(X_{\pi_m}^2) = E(f(E(X_{\pi_m} | X_{1:(m-1)})))$が成立する。
* 一方で、頂点$j \in \{\{ \pi_{m+1}, \dots, \pi_p\} \cap D\}$では、$E(X_j^2) > E(f(E(X_j | X_{1:(m-1)})))$となる。
* そのため、因果順序が$m$番目の要素$\pi_m$は、$E(X_j^2) = E(f(E(X_j | X_{1:(m-1)})))$となるような$j \in D$である。
* もし、そのような変数が存在しなければ、$X_{\pi_m}$は連続変数である。



#### $\pi_m = j \in C$の場合

* 仮定より、任意の頂点$k \in \{ \{ \pi_{m+1}, \dots, \pi_p \} \cap C \}$について、以下が成立する。

$$
\begin{align*}
    E(\mathit{Var}(X_{\pi_m} | X_{1:(m-1)}))
        &= \sigma_{\pi_m}^2 \\
        &< \sigma_k^2 + E(\mathit{Var}(E(X_k | X_{Pa(k)}) | X_{1:(m-1)})) \\
        &= E(E(\mathit{Var}(X_k | X_{Pa(k)}) | X_{1:(m-1)})) + E(\mathit{Var}(E(X_k | X_{Pa(k)}) | X_{1:(m-1)})) \\
        &= E(\mathit{Var}(X_k | X_{1:(m-1)}))
\end{align*}
$$

* よって、因果順序が$m$番目の要素$\pi_m$を特定することができる。





* 正規化？



